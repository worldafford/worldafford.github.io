<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>GenTron</title>
<link href="./template/style.css" rel="stylesheet">
<link rel="icon" type="image/x-icon" href="./assets/gentron.ico">
<script type="text/javascript" src="./template/jquery.mlens-1.0.min.js"></script> 
<script type="text/javascript" src="./template/jquery.js"></script>
</head>
<body>
<div class="content">
  <h1><strong>GenTron: Diffusion Transformers for Image and Video Generation</strong></h1>
  <h2 style="text-align: center;"><strong><a href="http://arxiv.org/abs/2312.04557" target="_blank">[CVPR 2024]</a></strong></h2>
  <p id="authors"><a href="https://www.shoufachen.com/">Shoufa Chen</a><sup>1, 2*</sup> <a href="https://mengmengxu.netlify.app/">Mengmeng Xu</a><sup>2*</sup> <a href="https://jiawei-ren.github.io/">Jiawei Ren</a><sup>2</sup> <a href="https://yrcong.github.io/">Yuren Cong</a><sup>2</sup>  <a href="https://senhe.github.io/"> Sen He</a><sup>2</sup></p>
  <p id="authors"><a href="https://uk.linkedin.com/in/yanping-xie-53583428">Yanping Xie</a><sup>2</sup> <a href="http://animesh-sinha.com/">Animesh Sinha</a><sup>2</sup> <a href="http://luoping.me/">Ping Luo</a><sup>1</sup> <a href="https://www.surrey.ac.uk/people/tao-xiang">Tao Xiang</a><sup>2</sup> <a href="https://scholar.google.com/citations?user=Vbvimu4AAAAJ&hl=es"> Juan-Manuel Perez-Rua</a><sup>2</sup>
  <br>
  <br>
  <span style="font-size: 22px; padding-right: 20px;"><sup>1</sup> The University of Hong Kong</span><span style="font-size: 22px; padding-left: 20px;"><sup>2</sup> Meta</span>
  <br>
  <span style="font-size: 18px;"><sup>*</sup> Equal contribution</span></p>
  <br>
  <!-- <img src="./assets/teaser.png" class="teaser-gif" style="width:90%;"><br> -->
</div>
  <div class="content">
    <h2>GenTron-T2V</h2>
    <br>
    <div class="image-row" style="text-align: center; margin-left: auto; margin-right: auto;">
      <figure style="display: inline-block; margin-right: 20px; width: 20%;">
        <img class="summary-img" src="./assets/t2v_demo1.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">A fantasy landscape trending on Artstation, 4k</figcaption>
      </figure>
      <figure style="display: inline-block; width: 20%">
        <img class="summary-img" src="./assets/t2v_demo2.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">An astronaut flying in space, 4k high resolution</figcaption>
      </figure>
      <figure style="display: inline-block; width: 20%">
        <img class="summary-img" src="./assets/t2v_demo3.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">A panda standing on a surfboard in the ocean in sunset, 4k</figcaption>
      </figure>
      <br>
      
      <figure style="display: inline-block; margin-right: 20px; width: 20%;">
        <img class="summary-img" src="./assets/t2v_demo4.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">An astronaut riding a horse high definition, 4k</figcaption>
      </figure>
      <figure style="display: inline-block; width: 20%">
        <img class="summary-img" src="./assets/t2v_demo5.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">Flying through a fantasy landscape, 4k high resolution</figcaption>
      </figure>
      <figure style="display: inline-block; width: 20%">
        <img class="summary-img" src="./assets/t2v_demo6.gif" style="width:100%;">
        <figcaption style="text-align: center; font-size: 12px;">Traveler walking alone in the misty forest at sunset, 4k</figcaption>
      </figure>
    </div>
  </div>

<div class="content">
  <h2 style="text-align:center;">Abstract</h2 >
  <p>In this study, we explore Transformer-based diffusion models for image and video generation.
    Despite the dominance of Transformer architectures in various fields due to their flexibility and scalability, the visual generative domain primarily utilizes CNN-based U-Net architectures, particularly in diffusion-based models. 
    We introduce <strong>GenTron</strong>, a family of <strong>Gen</strong>erative models employing <strong>Tr</strong>ansformer-based diffusi<strong>on</strong>, to address this gap.
    Our initial step was to adapt Diffusion Transformers (DiTs) from class to text conditioning, a process involving thorough empirical exploration of the conditioning mechanism.
    We then scale GenTron from approximately 900M to over 3B parameters, observing significant improvements in visual quality.
    Furthermore, we extend GenTron to text-to-video generation, incorporating novel motion-free guidance to enhance video quality.
    In human evaluations against SDXL, GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate). GenTron also excels in the T2I-CompBench, underscoring its strengths in compositional generation.  We believe this work will provide meaningful insights and serve as a valuable reference for future research.</p>
</div>

<div class="content">
  <h2>GenTron-T2I Results: T2I-CompBench</h2>
  <table>
    <tr>
        <td rowspan="2">Model</td>
        <td colspan="3">Attribute Binding</td>
        <td colspan="2">Object Relationship</td>
        <td rowspan="2">Complex</td>
        <td rowspan="2"><strong>Mean</strong></th>
    </tr>
    <tr>
        <td>Color</td>
        <td>Shape</td>
        <td>Texture</td>
        <td>Spatial</td>
        <td>Non-spatial</td>
    </tr>
    <!-- Add your table data here -->
    <tr>
      <td>SD v1.4</td> <td>37.65</td> <td>35.76</td> <td>41.56</td> <td>12.46</td> <td>30.79</td> <td>30.80</td> <td>31.50</td>
    </tr>
    <tr>
      <td>SD v2</td> <td>50.65</td> <td>42.21</td> <td>49.22</td> <td>13.42</td> <td>30.96</td> <td>33.86</td> <td>36.72</td>
    </tr>
    <tr>
      <td>Composable v2</td> <td>40.63</td> <td>32.99</td> <td>36.45</td> <td>8.00</td> <td>29.80</td> <td>28.98</td> <td>29.47</td>
    </tr>
    <tr>
      <td>Structured v2</td> <td>49.90</td> <td>42.18</td> <td>49.00</td> <td>13.86</td> <td>31.11</td> <td>33.55</td> <td>36.60</td>
    </tr>
    <tr>
      <td>Attn-Exct v2</td> <td>64.00</td> <td>45.17</td> <td>59.63</td> <td>14.55</td> <td>31.09</td> <td>34.01</td> <td>41.41</td>
    </tr>
    <tr>
      <td>GORS</td> <td>66.03</td> <td>47.85</td> <td>62.87</td> <td>18.15</td> <td>31.93</td> <td>33.28</td> <td>43.35</td>
    </tr>
    <tr>
      <td>DALLÂ·E 2</td> <td>57.50</td> <td>54.64</td> <td>63.74</td> <td>12.83</td> <td>30.43</td> <td>36.96</td> <td>42.68</td>
    </tr>
    <tr>
      <td>SD XL</td> <td>63.69</td> <td>54.08</td> <td>56.37</td> <td>20.32</td> <td>31.10</td> <td>40.91</td> <td>44.41</td>
    </tr>
    <tr>
      <td>PixArt-alpha</td> <td>68.86</td> <td>55.82</td> <td>70.44</td> <td>20.82</td> <td>31.79</td> <td>41.17</td> <td>48.15</td>
    </tr>
    <tr>
      <td><strong>GenTron</strong></td> <td><strong>76.74</strong></td> <td><strong>57.00</strong></td> <td><strong>71.50</strong></td> <td><strong>20.98</strong></td> <td><strong>32.02</strong></td> <td><strong>41.67</strong></td> <td><strong>49.99</strong></td>
    </tr>
  </table>
  <p>We present the alignment evaluation outcomes derived from T2I-CompBench. Our methodology exhibits exceptional efficacy across various domains, notably in attribute binding, object relationships, and intricate compositions. This signifies an advanced capability in compositional generation, particularly excelling in color binding.</p>
</div>

<div class="content">
  <h2>GenTron-T2I Results: User Study</h2>
  <br>
  <br>
  <img src="./assets/user_study.png" class="teaser-gif" style="width:60%;"><br>

  <p>We illustrate the comparison of user preferences between our method and SDXL.
    We generated 100 images using each method with standard prompts from <a href="https://github.com/google-research/parti/blob/main/PartiPrompts.tsv"  style="text-decoration: none;">PartiPrompt2</a>.
    GenTron achieves a 51.1% win rate in visual quality (with a 19.8% draw rate), and a 42.3% win rate in text alignment (with a 42.9% draw rate).</p>
</div>

<div class="content">
  <h2>Approach</h2>
  <div class="image-row" style="text-align: center;">
    <figure style="display: inline-block; margin-right: 20px; width: 20%;">
      <img class="summary-img" src="./assets/t2i_arch.png" style="width:80%;">
      <figcaption style="text-align: center; font-size: 12px;">GenTron-T2I Architecture</figcaption>
    </figure>
    <figure style="display: inline-block; width: 30%">
      <img class="summary-img" src="./assets/t2v_arch.png" style="width:90%;">
      <figcaption style="text-align: center; font-size: 12px;">GenTron-T2V Architecture</figcaption>
    </figure>
  </div>
  <p>For GenTron-T2V, the temporal self-attention layer is inserted between the cross-attention and the MLPs. The motion-free mask, which is an identity matrix, will be utilized in the <strong>TempSelfAttn</strong> with a probability of <span style="color: #365694;">p<sub>motion_free</sub></span>.</p>
</div>

<div class="content">
  <h2>Additional GenTron-T2I Results</h2>
  <br>
  <div class="image-row" style="text-align: center; margin-left: auto; margin-right: auto;">
    <figure class="image-figure" style="display: inline-block; margin-right: 20px; width: 20%;">
      <img class="summary-img" src="./assets/t2i_demo05.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">A cute happy Corgi playing in park, sunset, 4k</figcaption>
    </figure>
    <figure class="image-figure" style="display: inline-block; width: 20%">
      <img class="summary-img" src="./assets/t2i_demo04.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">A cute cat running </figcaption>
    </figure>
    <figure class="image-figure" style="display: inline-block; width: 20%">
      <img class="summary-img" src="./assets/t2i_demo02.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">A blue otter wearing a hat and dancing on the beach.</figcaption>
    </figure>
    <br>
    
    <figure class="image-figure" style="display: inline-block; margin-right: 20px; width: 20%;">
      <img class="summary-img" src="./assets/t2i_demo03.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">A car moving slowly on an empty street, rainy evening, van Gogh painting.</figcaption>
    </figure>
    <figure class="image-figure" style="display: inline-block; width: 20%">
      <img class="summary-img" src="./assets/t2i_demo07.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">Snow mountain and tree reflection in the lake</figcaption>
    </figure>
    <figure class="image-figure" style="display: inline-block; width: 20%">
      <img class="summary-img" src="./assets/t2i_demo08.png" style="width:100%;">
      <figcaption style="text-align: center; font-size: 12px;">a tiger in a field</figcaption>
    </figure>
  </div> 
</div>


<div class="content">
  <h2>BibTex</h2>
  <div class="code-container">
    <code>
@article{chen2023gentron,
  title={GenTron: Delving Deep into Diffusion Transformers for Image and Video Generation},
  author={Chen, Shoufa and Xu, Mengmeng and Ren, Jiawei and Cong, Yuren and He, Sen and Xie, Yanping and Sinha, Animesh and Luo, Ping and Xiang, Tao and Perez-Rua, Juan-Manuel},
  journal={arXiv preprint arXiv:2312.04557},
  year={2023}
} </code>
  </div>
</div>


<div class="content" id="acknowledgements">
  <p>Acknowledgements:
    The webpage template is borrowed from <a href="https://dreambooth.github.io/"  style="text-decoration: none;">DreamBooth</a>. We thank the authors for their codebase.
  </p>
</div>
</body>
</html>

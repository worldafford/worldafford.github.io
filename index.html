
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Worldafford:Affordance Grounding based on Natural Language Instructions</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Worldafford:Affordance Grounding based on Natural Language Instructions</h1>
                    <h3 class="title is-4 conference-authors"> 2024</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                Changmao&#160;Chen<sup>1</sup>,
                Yuren&#160;Cong</a><sup>2</sup>,
                Zhen Kan</a><sup>1</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>University of Science and Technology of China; </span>
                        <span class="author-block"><sup>2</sup>TNT, Leibniz University Hannover; </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2405.12461"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="assets/worldafford_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(coming soon)</span>
                </a>
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset(coming soon)</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
     Affordance grounding aims to localize the interaction regions
for the manipulated objects in the scene image according to given instructions, which is essential for Embodied AI and manipulation tasks.
A critical challenge in affordance grounding is that the embodied agent
should understand human instructions and analyze which tools in the
environment can be used, as well as how to use these tools to accomplish the instructions. Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing
to capture complex human objectives. Moreover, these approaches typically identify affordance regions of only a single object in object-centric
images, ignoring the object context and struggling to localize affordance
regions of multiple objects in complex scenes for practical applications.
To address this concern, for the first time, we introduce a new task of
affordance grounding based on natural language instructions, extending
it from previously using simple labels for complex human instructions.
For this new task, we propose a new framework, WorldAfford. We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically.
Subsequently, we use SAM and CLIP to localize the objects related to the
affordance knowledge in the image. We identify the affordance regions of
the objects through an affordance region localization module. To benchmark this new task and validate our framework, an affordance grounding
dataset, LLMaFF, is constructed. We conduct extensive experiments to
verify that WorldAfford performs state-of-the-art on both the previous
AGD20K and the new LLMaFF dataset. In particular, WorldAfford can
localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given
instruction.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <span style="font-size: 115%">The difference between our work and previous affordance grounding methods.</span>
                    <img src="assets/images/singe_action_label.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <!-- <span style="font-size: 110%"><b>Multimodal prompts for task specification.</b> We observe that many robot manipulation tasks can be expressed as <i>multimodal prompts</i> that interleave language and image/video frames. We propose VIMA, an embodied agent capable of processing mulitimodal prompts (left) and controlling a robot arm to solve the task (right).</span> -->
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Overview of MP5 architecture.</span></h2>
                    <img src="assets/images/pipeline.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
<span style="font-weight: bold">Module interaction in MP5.</span> After receiving the task instruction, MP5 first utilizes Parser to generate a sub-objective list. Once a sub-objective is passed to the Planner, the Planner Obtaining Env. Info. for Perception-aware Planning. The performer takes frequently Perception-aware Execution to interact with the environment by interacting with the Patroller. Both Perception-aware Planning and Execution rely on the Active Perception between the Percipient and the Patroller. Once there are execution failures, the Planner will re-schedule the action sequence of the current sub-objective. Mechanisms for collaboration and inspection of multiple modules guarantee the correctness and robustness when MP5 is solving an open-ended embodied task.</span>
                </div>
            </div>

        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Active Perception</span></h2>
                    <img src="assets/images/active perception.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                    <br>
                    <span style="font-size: 110%">
<span style="font-weight: bold">A demonstration of the process of Active Perception scheme.</span> Temporary Env. Info. Set saves information collected in the current scenario, so it should be reset at the beginning of Active Perception scheme. Performer then invokes Patroller to start asking Percipient questions with respect to the description of the sub-objective and the current execution action round by round. The responses of Percipient are saved in Temporary Env. Info. Set and are also gathered as the context for the next question-answering round. After finishing asking all significant necessary questions, Patroller will check whether the current execution action is complete by analyzing the current sub-objective with Perceived env info. saved in Temporary Env. Info. Set, therefore complex Context-Dependent Tasks could be solved smoothly.</span>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Experiments of Different Tasks</span></h2>

                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Process-Dependent Task Definition</span></h4>
                                                <span style="font-size: 110%">Process-Dependent Tasks primarily investigate situation-aware planning and embodied action execution, incorporating contributions from Active Perception and other modules that continuously perceive the environment and dynamically adjust their actions to accomplish long-horizon tasks. 
                                                <br>
                                                <span style="font-size: 100%">In the table below, we list the names of all tasks in Process-Dependent Tasks, their reasoning steps, object icons, the final recipe, and the required tools/platforms. The reasoning step refers to the number of sub-objectives that need to be completed in order to finish the entire task.
                                                <img src="assets/images/process dependent task.png" class="interpolation-image"
                                                     alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                                                <br>
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>

                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Context-Dependent Task Definition</span></h4>
                                                <span style="font-size: 110%">Context-Dependent Tasks primarily study how Active Perception enables the agent to better perceive low-level context information in the environment.
                                                <br>
                                                <span style="font-size: 100%">We first establish 6 aspects of environmental information derived from the Minecraft game environment: [Object, Mob, Ecology, Time, Weather, Brightness]. Each aspect has multiple options. Based on this, we define 16 tasks and organize their difficulty into 4 levels by taking into account the number of information elements that require perception as is shown in the table below. 
                                                <img src="assets/images/context dependent task1.png" class="interpolation-image"
                                                     alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                                                <br>
                                                <span style="font-size: 100%">Easy tasks necessitate the perception of only one element, Mid tasks include 2 perception elements, Hard tasks contain 3 elements, whereas Complex tasks involve the perception of 4 to 6 elements. Each task at the same level has different environment information content, the amount of environment information contained in each task, and the corresponding specific environment information is shown in the table below.
                                                <img src="assets/images/context dependent task2.png" class="interpolation-image"
                                                     alt="" style="display: block; margin-left: auto; margin-right: auto"/>
                                                <br>
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>

                            
                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Agent in Context-Dependent Task</span></h4>
                                                <video poster="" id="same_color" autoplay controls muted loop height="100%">
                                                    <source src="assets/videos/Video 2 new.mp4"
                                                            type="video/mp4">
                                                </video>
                                                <br>
                                                <!-- <span style="font-size: 110%">
                                                    The game-playing steps corresponding to the acquisition of different milestone objects by the agent during the completion of the craft diamond pickaxe challenge. The varying background colors denote the level of the Process-Dependent Tasks in which the milestone objects are located. -->
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>
                            
                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Agent in Process-Dependent Task</span></h4>
                                                <video poster="" id="same_color" autoplay controls muted loop height="100%">
                                                    <source src="assets/videos/Video 1.mp4"
                                                            type="video/mp4">
                                                </video>
                                                <br>
                                                <!-- <span style="font-size: 110%">
                                                    The game-playing steps corresponding to the acquisition of different milestone objects by the agent during the completion of the craft diamond pickaxe challenge. The varying background colors denote the level of the Process-Dependent Tasks in which the milestone objects are located. -->
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>

                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Agent in Open-Ended Task</span></h4>
                                                <video poster="" id="same_color" autoplay controls muted loop height="100%">
                                                    <source src="assets/videos/Video 3.mp4"
                                                            type="video/mp4">
                                                </video>
                                                <br>
                                                <!-- <span style="font-size: 110%">
                                                    The game-playing steps corresponding to the acquisition of different milestone objects by the agent during the completion of the craft diamond pickaxe challenge. The varying background colors denote the level of the Process-Dependent Tasks in which the milestone objects are located. -->
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>
                    
                </div>
            </div>

        </div>
    </div>
</section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
                        In this paper, we propose a novel multi-modal embodied system termed MP5 which is driven by frequently ego-centric scene perception for task planning and execution. In practice, it is designed by integrating five functional modules to accomplish task planning and execution via actively acquiring essential visual information from the scene. The experimental results suggest that our system represents an effective integration of perception, planning, and execution, skillfully crafted to handle both context- and process-dependent tasks within an open-ended environment.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<!-- <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{jiang2023vima,
  title     = {VIMA: General Robot Manipulation with Multimodal Prompts},
  author    = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  booktitle = {Fortieth International Conference on Machine Learning},
  year      = {2023}
}</code></pre>
    </div>
</section> -->

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{qin2023mp5,
  title={MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception},
  author={Yiran Qin and Enshen Zhou and Qichang Liu and Zhenfei Yin and Lu Sheng and Ruimao Zhang and Yu Qiao and Jing Shao},
  booktitle={arXiv preprint arxiv:2312.07472},
  year={2023}
}</code></pre>
    </div>
</section>


</body>
</html>

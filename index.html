
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Worldafford:Affordance Grounding based on Natural Language Instructions</title>

    <script>
        var task_map = {
            "simple-object-manipulation": "simple_object_manipulation",
            "visual-goal-reaching": "visual_goal_reaching",
            "novel-concept-grounding": "novel_concept_grounding",
            "one-shot-video-imitation": "one_shot_video_imitation",
            "visual-constraint-satisfaction": "visual_constraint_satisfaction",
            "visual-reasoning": "visual_reasoning"
        };

        function updateDemoVideo(category) {
            // var demo = document.getElementById("single-menu-demos").value;
            var task = document.getElementById(category + "-menu-tasks").value;
            var inst = document.getElementById(category + "-menu-instances").value;

            console.log(task_map[category], task, inst)

            var video = document.getElementById(category + "-single-task-video");
            video.src = "assets/videos/demos/" +
                task_map[category] +
                "/" +
                task +
                "/" +
                inst +
                ".mp4";
            video.playbackRate = 2.0;
            video.play();
        }
    </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
          rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="./static/css/academicons.min.css">
    <link rel="stylesheet"
          href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Worldafford:Affordance Grounding based on Natural Language Instructions</h1>
                    <h3 class="title is-4 conference-authors"> 2024</a>
                    </h3>
                    <div class="is-size-5 publication-authors">
            <span class="author-block">
                Changmao&#160;Chen<sup>1</sup>,
                Yuren&#160;Cong</a><sup>2</sup>,
                Zhen Kan</a><sup>1</sup>
            </span>
                    </div>

                    <div class="is-size-5 publication-authors">
                        <span class="author-block"><sup>1</sup>University of Science and Technology of China; </span>
                        <span class="author-block"><sup>2</sup>TNT, Leibniz University Hannover; </span>
                    </div>

                    <div class="column has-text-centered">
                        <div class="publication-links">
                            <!-- TODO PDF Link. -->
                            <span class="link-block">
                <a target="_blank" href="https://arxiv.org/abs/2405.12461"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

                            <span class="link-block">
                <a target="_blank" href="assets/worldafford_paper.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>PDF</span>
                </a>
              </span>
                            <!-- Code Link. -->
                            <span class="link-block">
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code(coming soon)</span>
                </a>
                <a target="_blank" href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Dataset(coming soon)</span>
                </a>
              </span>
                        </div>

                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <img src="assets/images/differnece.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto;"/>
                    <br>
                    <!-- <span style="font-size: 110%"><b>Multimodal prompts for task specification.</b> We observe that many robot manipulation tasks can be expressed as <i>multimodal prompts</i> that interleave language and image/video frames. We propose VIMA, an embodied agent capable of processing mulitimodal prompts (left) and controlling a robot arm to solve the task (right).</span> -->
                </div>
            </div>
        </div>
    </div>
</section>
    
<section class="section">
    <div class="container is-max-desktop">
        <!-- Abstract. -->
        <div class="columns is-centered has-text-centered">
            <div class="column">
                <h2 class="title is-3">Abstract</h2>
                <div class="content has-text-justified">
                    <p style="font-size: 125%">
     Affordance grounding aims to localize the interaction regions
for the manipulated objects in the scene image according to given instructions, which is essential for Embodied AI and manipulation tasks.
A critical challenge in affordance grounding is that the embodied agent
should understand human instructions and analyze which tools in the
environment can be used, as well as how to use these tools to accomplish the instructions. Most recent works primarily supports simple action labels as input instructions for localizing affordance regions, failing
to capture complex human objectives. Moreover, these approaches typically identify affordance regions of only a single object in object-centric
images, ignoring the object context and struggling to localize affordance
regions of multiple objects in complex scenes for practical applications.
To address this concern, for the first time, we introduce a new task of
affordance grounding based on natural language instructions, extending
it from previously using simple labels for complex human instructions.
For this new task, we propose a new framework, WorldAfford. We design a novel Affordance Reasoning Chain-of-Thought Prompting to reason about affordance knowledge from LLMs more precisely and logically.
Subsequently, we use SAM and CLIP to localize the objects related to the
affordance knowledge in the image. We identify the affordance regions of
the objects through an affordance region localization module. To benchmark this new task and validate our framework, an affordance grounding
dataset, LLMaFF, is constructed. We conduct extensive experiments to
verify that WorldAfford performs state-of-the-art on both the previous
AGD20K and the new LLMaFF dataset. In particular, WorldAfford can
localize the affordance regions of multiple objects and provide an alternative when objects in the environment cannot fully match the given
instruction.
                    </p>
                </div>
            </div>
        </div>
    </div>
</section>

<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">LLMaFF data collection pipeline.</span></h2>
                    <img src="assets/images/data_collection_process.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                    <br>
                    <!-- <span style="font-size: 110%"><b>Multimodal prompts for task specification.</b> We observe that many robot manipulation tasks can be expressed as <i>multimodal prompts</i> that interleave language and image/video frames. We propose VIMA, an embodied agent capable of processing mulitimodal prompts (left) and controlling a robot arm to solve the task (right).</span> -->
                </div>
            </div>
        </div>
    </div>
</section>

<!--Model-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Statistics of LLMaFF dataset.</span></h2>
                    <img src="assets/images/data_statistics.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                </div>
            </div>

        </div>
    </div>
</section>


<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span class="dvima">Method</span></h2>
                    <img src="assets/images/framework.png" class="interpolation-image"
                         alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                </div>
            </div>

        </div>
    </div>
</section>

<!--Experiments-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Experiments on two datasets</span></h2>

                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Qualitative results on AGD20K.</span></h4>
                                                <img src="assets/images/singe_action_label.png" class="interpolation-image"
                                                     alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                                                <br>
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>

                            <section class="section">
                                <div class="container is-max-widescreen">
                                    <div class="rows">
                                        <div class="rows is-centered ">
                                            <div class="row is-full-width">
                                                <h4 class="title is-4"><span class="dvima">Qualitative results on LLMaFF.</span></h4>
                                                <img src="assets/images/multi_objects.png" class="interpolation-image"
                                                     alt="" style="display: block; margin-left: auto; margin-right: auto; width: 100%;"/>
                                            </div>
                                        </div>
                            
                                    </div>
                                </div>
                            </section>

<!--Conclusion-->
<section class="section">
    <div class="container is-max-widescreen">
        <div class="rows">
            <div class="rows is-centered ">
                <div class="row is-full-width">
                    <h2 class="title is-3"><span
                            class="dvima">Conclusion</span></h2>

                    <p style="font-size: 125%">
In this paper, we introduce a new task of affordance grounding based on natural
language instructions and propose a novel framework, WorldAfford. Our framework uses LLMs to process natural language instructions and employs SAM and
CLIP for object segmentation and selection. We further propose a WeightedAffordance Grouding 17
Context Broadcasting module, allowing WorldAfford to localize affordance regions of multiple objects. Additionally, we present a new dataset, LLMaFF, to
benchmark this task. The experimental results demonstrate that WorldAfford
outperforms the other state-of-the-art methods for affordance grounding on both
the AGD20K dataset and the new LLMaFF dataset.
                    </p>

                </div>
            </div>

        </div>
    </div>
</section>


<!-- <section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@inproceedings{jiang2023vima,
  title     = {VIMA: General Robot Manipulation with Multimodal Prompts},
  author    = {Yunfan Jiang and Agrim Gupta and Zichen Zhang and Guanzhi Wang and Yongqiang Dou and Yanjun Chen and Li Fei-Fei and Anima Anandkumar and Yuke Zhu and Linxi Fan},
  booktitle = {Fortieth International Conference on Machine Learning},
  year      = {2023}
}</code></pre>
    </div>
</section> -->

<section class="section" id="BibTeX">
    <div class="container is-max-widescreen content">
        <h2 class="title">BibTeX</h2>
        <pre><code>@article{chen2024worldafford,
  title={WorldAfford: Affordance Grounding based on Natural Language Instructions},
  author={Chen, Changmao and Cong, Yuren and Kan, Zhen},
  journal={arXiv preprint arXiv:2405.12461},
  year={2024}
}
</code></pre>
    </div>
</section>


</body>
</html>
